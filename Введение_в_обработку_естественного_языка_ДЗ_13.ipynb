{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dZyeU8qnjAw7"
      },
      "source": [
        "# Генерация из GPT"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fz2iVM2rlQg-"
      },
      "source": [
        "Будем генерировать твиты по примерам из аккаунта \"Усы Пескова\": https://twitter.com/Sandy_mustache"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Se6JNcYPjAZd",
        "outputId": "66aea2a2-a7b6-4e41-9d7c-591c483bb16e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.25.1-py3-none-any.whl (5.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 5.8 MB 4.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.8/dist-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.8/dist-packages (from transformers) (6.0)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1\n",
            "  Downloading tokenizers-0.13.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 7.6 MB 40.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.8/dist-packages (from transformers) (4.64.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from transformers) (3.8.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.8/dist-packages (from transformers) (1.21.6)\n",
            "Collecting huggingface-hub<1.0,>=0.10.0\n",
            "  Downloading huggingface_hub-0.11.1-py3-none-any.whl (182 kB)\n",
            "\u001b[K     |████████████████████████████████| 182 kB 67.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.8/dist-packages (from transformers) (2022.6.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.8/dist-packages (from huggingface-hub<1.0,>=0.10.0->transformers) (4.1.1)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging>=20.0->transformers) (3.0.9)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (2022.9.24)\n",
            "Installing collected packages: tokenizers, huggingface-hub, transformers\n",
            "Successfully installed huggingface-hub-0.11.1 tokenizers-0.13.2 transformers-4.25.1\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YZpCJfoBgODA",
        "outputId": "e442af91-84f9-476f-cfff-94b684345b6d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-12-04 13:08:07--  https://gist.githubusercontent.com/avidale/d3da0ded85a4a16db6eb84d8331638ce/raw/a188084e5ef37b43b01fef0534b55c865b9a569e/tweets.txt\n",
            "Resolving gist.githubusercontent.com (gist.githubusercontent.com)... 185.199.110.133, 185.199.109.133, 185.199.108.133, ...\n",
            "Connecting to gist.githubusercontent.com (gist.githubusercontent.com)|185.199.110.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 4659 (4.5K) [text/plain]\n",
            "Saving to: ‘tweets.txt’\n",
            "\n",
            "\rtweets.txt            0%[                    ]       0  --.-KB/s               \rtweets.txt          100%[===================>]   4.55K  --.-KB/s    in 0s      \n",
            "\n",
            "2022-12-04 13:08:07 (56.7 MB/s) - ‘tweets.txt’ saved [4659/4659]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget https://gist.githubusercontent.com/avidale/d3da0ded85a4a16db6eb84d8331638ce/raw/a188084e5ef37b43b01fef0534b55c865b9a569e/tweets.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ia8HOsLMgaua",
        "outputId": "9ad15015-a6bd-4c88-d8f8-5eada561e84c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "26\n",
            "Знаете, это тот случай, когда Россия может на 400% доверять своему партнеру\n",
            "Тонер, картриджи и много-много бумаги\n",
            "Ну это примерно как Пескову поручить расследовать информацию о его часах\n",
            "Но тут главное, чтобы тракторы и бульдозеры были в порядке - и продолжали качественно давить вредные иностранные продукты\n",
            "В сеть утекли кадры с репетиции следующей инаугурации Путина\n",
            "- Скажи что-нибудь сладкое?\n",
            "- Сахар\n",
            "- Не. Еще слаще\n",
            "- Мёд\n",
            "- ЕЩЕ СЛАЩЕ!!!\n",
            "- Бюджет!!!\n",
            "- Вооооот\n"
          ]
        }
      ],
      "source": [
        "with open('tweets.txt', 'r') as f:\n",
        "    tweets = f.read().strip().split('\\n\\n')\n",
        "print(len(tweets))\n",
        "for i in range(10, 16):\n",
        "    print(tweets[i])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "yxDPUmnJhAuu"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "device = torch.device(\"cuda\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "FeVUjitigoj4"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer, AutoModelForCausalLM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CFc42rc8g_wp",
        "outputId": "b5542949-f8c1-40b9-b0ec-b5d325ade917"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
          ]
        }
      ],
      "source": [
        "model_name = 'sberbank-ai/rugpt3large_based_on_gpt2'\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name).to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "I_rb4emjhL1s"
      },
      "outputs": [],
      "source": [
        "import random"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JvVleYTChNM8",
        "outputId": "bd5a91b1-acbb-4a91-fbac-1f3e9bb329ab"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "***\n",
            "Да. Еще очень многие помнят, что такое госплан, как планировалось, талоны на еду, очереди, дефицит, выездные визы. Но спасибо, что напомнил\n",
            "***\n",
            "Когда работа - улёт\n",
            "***\n",
            "Тонер, картриджи и много-много бумаги\n",
            "***\n",
            "- Скажи что-нибудь сладкое?\n",
            "- Сахар\n",
            "- Не. Еще слаще\n",
            "- Мёд\n",
            "- ЕЩЕ СЛАЩЕ!!!\n",
            "- Бюджет!!!\n",
            "- Вооооот\n",
            "***\n",
            "В сеть утекли кадры с репетиции следующей инаугурации Путина\n",
            "***\n",
            "\n"
          ]
        }
      ],
      "source": [
        "sep = '\\n***\\n'\n",
        "# sep = '\\n27479153\tSandy_mustache\t2021-02-18 16:44:00\t'\n",
        "\n",
        "prefix = sep.join([''] + random.sample(tweets, k=5) + [''])\n",
        "\n",
        "tokens = tokenizer(prefix, return_tensors='pt')\n",
        "tokens = {k: v.to(model.device) for k, v in tokens.items()}\n",
        "end_token_id = tokenizer.encode('***')[0]\n",
        "print(prefix)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LfsS4j48hXki",
        "outputId": "45495027-0c17-4ae1-c481-d5bbc0c9720c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Смотрю в окно: снег валит...<s>\n",
            "Праздник к нам приходит! \n",
            "\n",
            "А еще сегодня день рождения у моего друга. И я хочу поздравить его от всей души! Пусть он всегда будет таким же веселым, жизнерадостным и позитивным человеком, каким был до того, как попал в этот мир! Желаю ему всего самого наилучшего во всех его начинаниях и свершениях! \n",
            "Желаю тебе здоровья, счастья, любви и благополучия! Чтобы все твои мечты сбывались, а планы воплощались в жизнь! Чтоб ты никогда не чувствовал себя обделенным ни в чем! А главное, чтобы тебя окруж\n"
          ]
        }
      ],
      "source": [
        "size = tokens['input_ids'].shape[1]\n",
        "output = model.generate(\n",
        "    **tokens, \n",
        "    #end_token=end_token_id,\n",
        "    do_sample=False, \n",
        "    max_length=size+128, \n",
        "    repetition_penalty=4.2, \n",
        "    temperature=0.7,\n",
        "    num_beams=3,\n",
        ")\n",
        "decoded = tokenizer.decode(output[0])\n",
        "result = decoded[len(prefix):]\n",
        "print(result)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DjWvl0KnhpHJ"
      },
      "source": [
        "# Диалоги"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E3iWjkcapklq"
      },
      "source": [
        "Логика та же самая, что и с твитами. Только выбрали разделитель в стиле bash.org"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "BMp34fnqnPTT"
      },
      "outputs": [],
      "source": [
        "def respond_to_dialog(texts):\n",
        "    prefix = '\\nx:'\n",
        "    for i, t in enumerate(texts):\n",
        "        prefix += t\n",
        "        prefix += '\\nx:' if i % 2 == 1 else '\\ny:'\n",
        "    tokens = tokenizer(prefix, return_tensors='pt')\n",
        "    tokens = {k: v.to(model.device) for k, v in tokens.items()}\n",
        "    end_token_id = tokenizer.encode('\\n')[0]\n",
        "    size = tokens['input_ids'].shape[1]\n",
        "    output = model.generate(\n",
        "        **tokens, \n",
        "        eos_token_id=end_token_id,\n",
        "        do_sample=True, \n",
        "        max_length=size+128, \n",
        "        repetition_penalty=3.2, \n",
        "        temperature=1,\n",
        "        num_beams=3,\n",
        "        length_penalty=0.01,\n",
        "        pad_token_id=tokenizer.eos_token_id\n",
        "    )\n",
        "    decoded = tokenizer.decode(output[0])\n",
        "    result = decoded[len(prefix):]\n",
        "    return result.strip()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z3ONplkBnosw",
        "outputId": "c485b136-4e9e-49bc-84d8-84dbba5f4420"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Начните диалог с ботом любой фразой\n",
            "здарова\n",
            "привет\n",
            "ты кто\n",
            "я друг\n",
            "чей\n",
            "мой\n",
            "ты сам себе друг\n",
            "нет\n",
            "а кому\n",
            "мне\n",
            "пока\n"
          ]
        }
      ],
      "source": [
        "seed = input('Начните диалог с ботом любой фразой\\n')\n",
        "history = [seed]\n",
        "while True:\n",
        "    result = respond_to_dialog(history[-10:])\n",
        "    next_sentence = input(result + '\\n')\n",
        "    if next_sentence == 'пока': break\n",
        "    history.append(result)\n",
        "    history.append(next_sentence)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cw70GdX5mh8b",
        "outputId": "77ba384e-fe0b-4a7e-dda6-45a32409b375"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "А давай!\n",
            "Каких?\n",
            "О, да! А чем ты занимаешься?\n",
            "О, да. У тебя есть собака?\n",
            "О, у меня есть собака!\n",
            "Каких?\n",
            "У тебя есть кошка?\n",
            "Я не люблю собак\n",
            "О каких?\n",
            "Каких?\n"
          ]
        }
      ],
      "source": [
        "for i in range(10):\n",
        "    print(respond_to_dialog(['Давай поговорим о домашних животных']))"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.8.10 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.10"
    },
    "vscode": {
      "interpreter": {
        "hash": "c04fbba28c5fbfaf43d34390ac7e98e34943c1430afb322213ae62fcdb8e13f2"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}